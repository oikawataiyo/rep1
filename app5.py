import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Streamlitã®ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="My Great ChatGPT",
    page_icon="ğŸ¤—"
)
st.header("My Great ChatGPTğŸ¤—")

container = st.container() 
with container:
    with st.form(key='my_form', clear_on_submit=True):
        user_input = st.text_area(label='Message: ', key='input', height=100)
        submit_button = st.form_submit_button(label='Send')

    if submit_button and user_input: 
        # é€ä¿¡ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãŒã‚ã‚Œã°å®Ÿè¡Œ
        st.write(f"å…¥åŠ›ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {user_input}")
        
        # ChatGPTã®å›ç­”ã‚’å–å¾—ã™ã‚‹å‡¦ç†
        llm = ChatOpenAI()

        # ChatGPTã¸ã®è³ªå•ã‚’å‡¦ç†ã™ã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
        prompt = ChatPromptTemplate.from_messages([ 
            ("system", "You are a helpful assistant."),  # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            ("user", user_input)  # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
        ])

        # å‡ºåŠ›ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ãŸã‚ã®å‡¦ç†ã‚’ä½œæˆ
        output_parser = StrOutputParser()

        # é€£ç¶šçš„ãªå‡¦ç†ã‚’ä½œæˆ
        chain = prompt | llm | output_parser

        # è³ªå•ã‚’ChatGPTã«æ¸¡ã—ã¦å›ç­”ã‚’å–å¾—
        response = chain.invoke({"input": user_input})

        # ChatGPTã®è¿”ç­”ã‚’è¡¨ç¤º
        st.write(f"ChatGPTã®è¿”ç­”: {response}")

#ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤º
st.sidebar.title("Options")

#ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³ã‚’è¨­ç½®
model = st.sidebar.radio("Choose a model:", ("GPT-3.5", "GPT-4"))

#ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒœã‚¿ãƒ³ã‚’è¨­ç½®
clear_button = st.sidebar.button("Clear Conversation", key="clear")

#ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã€temperatureã‚’0ã‹ã‚‰2ã¾ã§ã®ç¯„å›²ã§é¸æŠå¯èƒ½ã«ã™ã‚‹
#åˆæœŸå€¤ã¯0.0ã€åˆ»ã¿å¹…ã¯0.1ã¨ã™ã‚‹
temperature = st.sidebar.slider("Temperature:", min_value=0.0, max_value=2.0, value=0.0, step=0.1)

# Streamlitã¯markdown ã‚’æ›¸ã‘ã°ã„ã„æ„Ÿã˜ã«HTMLã§è¡¨ç¤ºã—ã¦ãã‚Œã¾ã™
#(markdownã¯ã‚‚ã¡ã‚ã‚“ã‚µã‚¤ãƒ‰ãƒãƒ¼ä»¥å¤–ã®ç®‡æ‰€ã§ã‚‚ä½¿ãˆã¾ã™)
st.sidebar.markdown("## Costs")
st.sidebar.markdown("**Total cost**")
st.sidebar.markdown("- Input cost: $0.001 ") # dummy
st.sidebar.markdown("- Output cost: $0.001 ") # dummy
def select_model():
    #ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã€temperatureã‚’0ã‹ã‚‰2ã¾ã§ã®ç¯„å›²ã§é¸æŠå¯èƒ½ã«ã™ã‚‹
    #åˆæœŸå€¤ã¯0.0ã€åˆ»ã¿å¹…ã¯0.01ã¨ã™ã‚‹
    temperature = st.sidebar.slider(
        "Temperature:", min_value=0.0, max_value=2.0, value=0.0, step=0.01)

    models = ("GPT-3.5", "GPT-4")
    model = st.sidebar.radio("Choose a model:", models)
    if model == "GPT-3.5":
        st.session_state.model_name = "gpt-3.5-turbo" 
        return ChatOpenAI (
            temperature=temperature, 
            model_name=st.session_state.model_name
        )
    elif model == "GPT-4":
        st.session_state.model_name = "gpt-40"
        return ChatOpenAI(
            temperature=temperature,
            model_name=st.session_state.model_name
        )

def init_chain():
    llm = select_model()

def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    #clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    if clear_button or "message_history" not in st.session_state:
       st.session_state.message_history = [
           ("system", "You are a helpful assistant.")
       ]

def main():
    init_page()
    init_messages()

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_messages([
    ("user", "{user_input}"), # inputã«ã¯ã‚ã¨ã§user_inputãŒä»£å…¥ã•ã‚Œã‚‹
])
llm = ChatOpenAI (temperature=0)
output_parser = StrOutputParser()
chain = prompt | llm | output_parser

response = chain.invoke({"user_input": "ã“ã‚“ã«ã¡ã¯"})
print(response)
#=> ã“ã‚“ã«ã¡ã¯! ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹?!

#ä¸€åº¦ã«è¤‡æ•°ã®å…¥åŠ›ã‚’å‡¦ç†ã™ã‚‹å ´åˆ
responses = chain.batch([
    {"user_input": "ã“ã‚“ã«ã¡ã¯"},
    {"user_input": "ä»Šæ—¥ã®å¤©æ°—ã¯?"},
    {"user_input": "æ˜æ—¥ã®äºˆå®šã¯?"}
])

print(responses)
# => [
#     'ã“ã‚“ã«ã¡ã¯! ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹?',
#     'ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ç§ã¯å¤©æ°—æƒ…å ±ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚',
#     'ç§ã¯AIã§ã™ã®ã§ã€æ˜æ—¥ã®äºˆå®šã¯ã‚ã‚Šã¾ã›ã‚“ã€‚',
#]

#max_concurrency ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€å¤§ä¸¦åˆ—æ•°ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚å¯èƒ½
responses = chain.batch([
    {"user_input": "ã“ã‚“ã«ã¡ã¯"},
    {"user_input": "ä»Šæ—¥ã®å¤©æ°—ã¯?"},
    {"user_input": "æ˜æ—¥ã®äºˆå®šã¯?"},
    {"user_input":"æ˜å¾Œæ—¥ã®äºˆå®šã¯?"},
    {"user_input":"æ˜ã€…å¾Œæ—¥ã®äºˆå®šã¯?"}
], config={"max_concurrency": 3}
)

response = st.write_stream(chain.stream({"user_input": user_input}))

import streamlit as st

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "message_history" not in st.session_state:
    st.session_state.message_history = []

def main():
    # 1. ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    for role, message in st.session_state.get("message_history", []):
        st.chat_message(role).markdown(message)

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’ç›£è¦–
    if user_input := st.chat_input("èããŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ã­!"):
        st.chat_message("user").markdown(user_input)

        # 2. LLMã®è¿”ç­”ã‚’ Streaming è¡¨ç¤ºã™ã‚‹
        with st.chat_message("ai"):
            # ä¾‹ã¨ã—ã¦ LLM ã®è¿”ç­”ã‚’ç”Ÿæˆã™ã‚‹ç®‡æ‰€ã‚’æ“¬ä¼¼çš„ã«è¡¨ç¾
            response_placeholder = st.empty()  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ç”¨æ„
            response = ""
            for chunk in chain.stream({"user_input": user_input}):  # ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†
                response += chunk  # ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
                response_placeholder.markdown(response)  # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°

        # 3. ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        st.session_state.message_history.append(("user", user_input))
        st.session_state.message_history.append(("ai", response))

if __name__ == "__main__":
    main()

# models
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic#è¿½åŠ 
from langchain_google_genai import ChatGoogleGenerativeAI#è¿½åŠ 
import openai
import streamlit as st
from anthropic import ChatAnthropic
from google.generativeai import ChatGoogleGenerativeAI

def select_model(model, temperature):
    if model == "GPT-3.5":
        st.session_state.model_name = "gpt-3.5-turbo"
        return openai.ChatCompletion.create(
            model=st.session_state.model_name,
            temperature=temperature,
            messages=[{"role": "system", "content": "You are a helpful assistant."}]
        )
    elif model == "GPT-4":
        st.session_state.model_name = "gpt-4"
        return openai.ChatCompletion.create(
            model=st.session_state.model_name,
            temperature=temperature,
            messages=[{"role": "system", "content": "You are a helpful assistant."}]
        )
    elif model == "Claude 3.5 Sonnet":
        st.session_state.model_name = "claude-3-5-sonnet-20240620"
        return ChatAnthropic(
            temperature=temperature,
            model_name=st.session_state.model_name
        )
    elif model == "Gemini 1.5 Pro":
        st.session_state.model_name = "gemini-1.5-pro-latest"
        return ChatGoogleGenerativeAI(
            temperature=temperature,
            model=st.session_state.model_name
        )

def init_chain(model, temperature):
    llm = select_model(model, temperature)
    return llm

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒ¢ãƒ‡ãƒ«é¸æŠç”¨ã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‚’è¿½åŠ 
st.sidebar.title("LLM åˆ‡ã‚Šæ›¿ãˆ")
model_options = ["GPT-3.5", "GPT-4", "Claude 3.5 Sonnet", "Gemini 1.5 Pro"]
model = st.sidebar.selectbox("ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ", model_options)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«æ¸©åº¦è¨­å®šã®ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ 
temperature = st.sidebar.slider("æ¸©åº¦", 0.0, 1.0, 0.7)

# ãƒ¢ãƒ‡ãƒ«ã¨æ¸©åº¦ã«åŸºã¥ã„ã¦LLMã‚’é¸æŠã—ã€åˆæœŸåŒ–ã™ã‚‹
llm = init_chain(model, temperature)

# ã“ã“ã§ã€é¸æŠã—ãŸLLMã‚’ä½¿ã£ã¦ã€ä»–ã®å‡¦ç†ã‚’é€²ã‚ã¾ã™
st.write(f"é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: {model}")

